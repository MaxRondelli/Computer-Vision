{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfALh63Qto-P"
      },
      "source": [
        "# Metric Learning\n",
        "credits to [Giuseppe Lisanti](https://www.unibo.it/sitoweb/giuseppe.lisanti/en), Samuele Salti and Riccardo Spezialetti\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFdC0S2gcdsE"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V1pFkeWhatzA"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils as utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import random\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from timeit import default_timer as timer\n",
        "from typing import Callable, Dict, List, Tuple, Union\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJbkbcfoyPFY"
      },
      "source": [
        "## Runtime Settings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ib8_3-Scxvk8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All good, a Gpu is available\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available:\n",
        "  print('All good, a Gpu is available')\n",
        "  device = torch.device(\"cuda:0\")  \n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPmh48qzlrzk"
      },
      "source": [
        "## Reproducibility & Deterministic mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhOiCAcW0R2i"
      },
      "outputs": [],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "fix_random(seed=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhLzk2hC_fE5"
      },
      "source": [
        "# Face Identification/Recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edVJ6DI1BVIP"
      },
      "source": [
        "Given images of faces, we can either **recognize the identity** of the people involved or **verify** they depict the same person. We can therefore efine two realted but different problems:\n",
        "\n",
        "**1 - Face Verification**: A one-to-one mapping where given two images we have to confirm that they depict the same person.\n",
        "\n",
        "**2 - Face Identification/Recognition**: A one-to-many mapping where given an image and a database of images of known faces (***gallery***), we have to identify the subject depicted in the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B3iISUSH1qa"
      },
      "source": [
        "## Face Detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZtKsg0RHrpz"
      },
      "source": [
        "To solve face detection, we will use a *face detector* based on neural networks to crop the face of the subject from the photo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z6KmrL5KRAJ"
      },
      "source": [
        "Install the ```facenet-pytorch``` [package](https://github.com/timesler/facenet-pytorch).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhMFBHM2KOHV"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTNN8yA2byDO"
      },
      "outputs": [],
      "source": [
        "def generate_colors(num_colors: int) -> np.array:\n",
        "    \"\"\"Generates an array with RGB triplets representing colors.\n",
        "\n",
        "    Args:\n",
        "        num_colors: the number of colors to generate.\n",
        "\n",
        "    Returns: \n",
        "        the generated colors.\n",
        "    \"\"\"\n",
        "    np.random.seed(0)\n",
        "    colors = np.random.uniform(0, 255, size=(num_colors, 3))\n",
        "    time_in_ms = 1000 * time.time()\n",
        "    np.random.seed(int(time_in_ms) % 2 ** 32)\n",
        "\n",
        "    return colors\n",
        "\n",
        "def draw_detection_results(image: Image, \n",
        "                           boxes: np.ndarray,\n",
        "                           landmarks: np.ndarray, \n",
        "                           colors: np.ndarray) -> Image:\n",
        "    \"\"\"Draws the detected bounding boxes and landmarks on image.\n",
        "\n",
        "    Args:\n",
        "        image: the input image.\n",
        "        boxes: the detected bounding boxes.\n",
        "        landmarks: the detected landmarks.\n",
        "        colors: the color to use to draw the bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "        The image with the annotations.        \n",
        "    \"\"\"\n",
        "    image_annotated = copy.deepcopy(image)\n",
        "    painter = ImageDraw.Draw(image_annotated)\n",
        "\n",
        "    for i, (box, point) in enumerate(zip(boxes, landmarks)):\n",
        "        color = tuple(colors[i].astype(np.int32))\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "        painter.rectangle([x_min, y_min, x_max, y_max], width=5, outline=color)\n",
        "        for p in point:\n",
        "            x, y = p\n",
        "            painter.rectangle([x - 10, y - 10, x + 10, y + 10], width=4, \n",
        "                              outline=color, \n",
        "                              fill=color)\n",
        "\n",
        "    return image_annotated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7XIFB0bfDr9"
      },
      "source": [
        "For face detection, we will use the *Multi-Task Cascaded Convolutional Neural Network (**MTCNN**)* presentend in the paper [Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks](https://arxiv.org/pdf/1604.02878.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyLpYd4icFKj"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "# Create a detector with default parameters\n",
        "size_image = 160\n",
        "detector = MTCNN(image_size=size_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZwv0yA2fF-"
      },
      "source": [
        "Let's try the detector and draw the results on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdYlJ7xRoHzb"
      },
      "outputs": [],
      "source": [
        "image_url = \"https://www.basketinside.com/wp-content/uploads/2016/07/magic-jordan-bird.jpg\"\n",
        "response = requests.get(image_url, stream=True)\n",
        "\n",
        "image = Image.open(response.raw).convert(\"RGB\")\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br7Hx8hpI91o"
      },
      "source": [
        "The `detect` method returns the coordinates of bounding boxes where the algorithm has detected the faces, a confidence score for each box and the coordinates of 5 face landmarks (or keypoints)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q76hqcjwKhCn"
      },
      "outputs": [],
      "source": [
        "boxes, scores, landmarks = detector.detect(image, landmarks=True)\n",
        "\n",
        "image_with_annotations = draw_detection_results(image, boxes, landmarks, \n",
        "                                                generate_colors(len(boxes)))\n",
        "\n",
        "plt.imshow(image_with_annotations)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CEwbmlJOQJe"
      },
      "outputs": [],
      "source": [
        "def crop_faces(image: Image, \n",
        "               boxes: np.ndarray,\n",
        "               margin: int, \n",
        "               size_face: int) -> List[PIL.Image.Image]:\n",
        "    \"\"\"Crops the pixel in the image corresponding to the bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        image: the input image.\n",
        "        boxes: the bounding boxes\n",
        "        margin: the margin to add to the bounding box, in terms of pixels in \n",
        "        the final image.\n",
        "        size_face: the output image size in pixels, the image will be square.\n",
        "\n",
        "    Returns:\n",
        "        The extracted faces from the image.\n",
        "    \"\"\"\n",
        "    faces = []\n",
        "    width, height = image.size\n",
        "    \n",
        "    for box in boxes:\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "        \n",
        "        x_margin = margin * (x_max - x_min) / (size_face - margin)\n",
        "        x_margin *= 0.5        \n",
        "        y_margin = margin * (y_max - y_min) / (size_face - margin)\n",
        "        y_margin *= 0.5\n",
        "\n",
        "        x_min = int(max(x_min - x_margin, 0))\n",
        "        y_min = int(max(y_min - y_margin, 0))\n",
        "        x_max = int(min(x_max + x_margin, width))\n",
        "        y_max = int(min(y_max + y_margin, height))\n",
        "\n",
        "        face = np.asarray(image)[y_min:y_max, x_min:x_max]\n",
        "        \n",
        "        face_image = Image.fromarray(face).resize((size_face, size_face), \n",
        "                                                  Image.BILINEAR)\n",
        "        faces.append(face_image)\n",
        "    return faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h85xTR9boUm-"
      },
      "outputs": [],
      "source": [
        "image_url = \"https://www.basketinside.com/wp-content/uploads/2016/07/magic-jordan-bird.jpg\"\n",
        "response = requests.get(image_url, stream=True)\n",
        "\n",
        "image = Image.open(response.raw).convert(\"RGB\")\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFoxZmdi2vdJ"
      },
      "outputs": [],
      "source": [
        "margin = 30\n",
        "\n",
        "boxes, scores = detector.detect(image, landmarks=False)\n",
        "faces = crop_faces(image, boxes, margin, 160)\n",
        "\n",
        "# Plotting \n",
        "figure = plt.figure(figsize=(15, 5))\n",
        "\n",
        "for idx, face in enumerate(faces):\n",
        "    figure.add_subplot(1, len(faces), idx + 1)\n",
        "    plt.title(f'Score: {scores[idx]:.3f}')\n",
        "    plt.imshow(face)\n",
        "    plt.axis(\"off\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obZXiQVPw0Ys"
      },
      "source": [
        "Let's create a face identification pipeline using the LFW dataset to build our gallery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jLtfTNJRDp"
      },
      "source": [
        "## Get the Labeled Faces in the Wild  [dataset](http://vis-www.cs.umass.edu/lfw/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96r1LebSnT8x"
      },
      "outputs": [],
      "source": [
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Pj3TlSPEmZ"
      },
      "outputs": [],
      "source": [
        "!tar -zxf lfw.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1mXIsxJnK5X"
      },
      "source": [
        "In face verification/identification, there should be no overlapping identities between the training set and testing set. This [repository](https://github.com/happynear/FaceDatasets) provides some overlapping lists between several training and testing datasets. In our case we need the overlapping list [CASIA-LFW](https://github.com/happynear/FaceDatasets/blob/master/CASIA/webface_lfw_overlap_detail.txt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHoJ5Wb33XwR"
      },
      "outputs": [],
      "source": [
        "!ls lfw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H5d2ztjrIe5"
      },
      "outputs": [],
      "source": [
        "overlap_casia_lfw = ['William_Macy', 'Vanessa_Williams', 'Mary_Blige', \n",
        "                    'Laura_Elena_Harring', 'David_Kelley', 'Norm_Macdonald', \n",
        "                    'Hillary_Clinton', 'Wanda_de_la_Jesus', 'Michael_Jordan', \n",
        "                    'Nicole_Parker', 'Zhang_Ziyi', 'Prince_William', 'Liu_Ye', \n",
        "                    'Randy_Jackson', 'Jesse_James', 'John_Mabry', 'Richard_Cohen']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWkxPGojpWmJ"
      },
      "outputs": [],
      "source": [
        "def remove_sub_folders(path_base: str, folders: str) -> int:\n",
        "    \"\"\"Removes all the sub folders inside the folder path_base.\n",
        "\n",
        "    Args:\n",
        "        path_base: the base path.\n",
        "        folders: the folder to delete.\n",
        "\n",
        "    Returns:\n",
        "        the number of deleted folders.\n",
        "\n",
        "    Raises:\n",
        "        OSError: if is impossible to delete the folder.\n",
        "    \"\"\"\n",
        "    num_deleted_folders = 0\n",
        "    for folder in folders:\n",
        "        path_folder = os.path.join(path_base, folder)\n",
        "        try:\n",
        "            shutil.rmtree(path_folder)\n",
        "            num_deleted_folders += 1\n",
        "        except OSError as e:\n",
        "            print(f'Impossible to delete folder {path_folder}')\n",
        "            raise\n",
        "    return num_deleted_folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdxQ_8curTyD"
      },
      "outputs": [],
      "source": [
        "remove_sub_folders('lfw', overlap_casia_lfw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7HVUxHfWh1A"
      },
      "outputs": [],
      "source": [
        "path_ds_lfw = \"lfw\"\n",
        "dataset_lfw = torchvision.datasets.ImageFolder(path_ds_lfw)\n",
        "\n",
        "identities = dataset_lfw.classes\n",
        "num_identities = len(identities)\n",
        "\n",
        "print(f'Images: {len(dataset_lfw)} - People: {num_identities}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eulWFTjejJBh"
      },
      "outputs": [],
      "source": [
        "index_sample = 2026 \n",
        "\n",
        "image, label = dataset_lfw[index_sample]\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(identities[label])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuW2ivnmvQGW"
      },
      "source": [
        "## Build the Model Gallery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9n4dbvaGOzy"
      },
      "source": [
        "We need to crop all the images before creating our embeddings gallery. Instead of cropping each single face from the image using the method shown above, we can use the [forward](https://github.com/timesler/facenet-pytorch/blob/dd0b0e4b5b124b599f75b87e570910e5d80c8848/models/mtcnn.py#L226) method of the MTCNN detector. However, it requires `PIL.Image`s as input, rather than `torch.tensors` as we have used so far. To create a batch of images, we can use\n",
        "the [collate_pil](https://github.com/timesler/facenet-pytorch/blob/master/models/utils/training.py#L139) provided by the package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76GdZyjOGTkj"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import training\n",
        "num_workers = 2\n",
        "size_batch = 16\n",
        "\n",
        "loader_lfw = torch.utils.data.DataLoader(dataset_lfw, \n",
        "                                         batch_size=size_batch,\n",
        "                                         pin_memory=True,\n",
        "                                         num_workers=num_workers,\n",
        "                                         collate_fn=training.collate_pil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKFJm3wJitt"
      },
      "source": [
        "The attribute `samples` of a PyTorch `torchvision.datasets.ImageFolder` stores the path to the image and the class labels and they are the two information collated to create a mini-batch by the dataloader. However, we do not need the label, but we need the path of the image, to save its cropped vesion with the same path in a new folder. Hence, we overwrite the label with the path to the image in the lfw dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsf7b7NmKSEz"
      },
      "outputs": [],
      "source": [
        "print(dataset_lfw.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q-RUnxFJjVt"
      },
      "outputs": [],
      "source": [
        "dataset_lfw.samples = [(path, path) for path, _ in dataset_lfw.samples]\n",
        "print(dataset_lfw.samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI6kOyvtKlXJ"
      },
      "source": [
        "Now we can detect faces on the lfw dataset and save the cropped dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfatVyBMNfG"
      },
      "outputs": [],
      "source": [
        "detector = MTCNN(image_size=size_image,\n",
        "                 margin=14,\n",
        "                 device=device,\n",
        "                 selection_method='center_weighted_size')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57pDcVFKj5Y"
      },
      "outputs": [],
      "source": [
        "path_ds_lfw_cropped = f'{path_ds_lfw }_cropped'\n",
        "\n",
        "for i, (x, paths_batch) in tqdm(enumerate(loader_lfw), total=len(loader_lfw)):\n",
        "    path_crop = [p.replace(path_ds_lfw, path_ds_lfw_cropped) for p in paths_batch]\n",
        "    detector(x, save_path=path_crop)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9Xf4tQfWB96"
      },
      "outputs": [],
      "source": [
        "dataset_lfw_cropped = torchvision.datasets.ImageFolder(path_ds_lfw_cropped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfJdjtJVXktD"
      },
      "outputs": [],
      "source": [
        "index_sample = 11413\n",
        "\n",
        "image, label = dataset_lfw_cropped[index_sample]\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(identities[label])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBRdqBNBWA9F"
      },
      "source": [
        "Now we can create the dataset and the data loader for the cropped images. As regard image normalization, we can use the `fixed_image_standardization` [method](https://github.com/timesler/facenet-pytorch/blob/dd0b0e4b5b124b599f75b87e570910e5d80c8848/models/mtcnn.py#L508), since the pre-trained models have been trained with the images normalized according to this [procedure](https://github.com/davidsandberg/facenet). Using the [`torch.utils.data.SequentialSampler`](https://pytorch.org/docs/stable/data.html) we will read the elements sequentially, always in the same order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0Ilfw9rXhtX"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import fixed_image_standardization\n",
        "\n",
        "transform_lfw = transforms.Compose([np.float32,\n",
        "                                    transforms.ToTensor(),\n",
        "                                    fixed_image_standardization])\n",
        "\n",
        "dataset_lfw_cropped = torchvision.datasets.ImageFolder(path_ds_lfw_cropped, \n",
        "                                                       transform=transform_lfw)\n",
        "\n",
        "sampler = torch.utils.data.SequentialSampler(dataset_lfw_cropped)\n",
        "loader_lfw_cropped = torch.utils.data.DataLoader(dataset_lfw_cropped,\n",
        "                                                 num_workers=num_workers,\n",
        "                                                 batch_size=size_batch,\n",
        "                                                 sampler=sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbvINBzdFhKa"
      },
      "source": [
        "Take the embedder pretrained on the Casia Web Face dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3atvhgJv2IL"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "embedder = InceptionResnetV1(pretrained='casia-webface').eval()\n",
        "embedder.to(device)\n",
        "summary(embedder, input_size=(3, 160, 160))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfSzgx6kdkZC"
      },
      "source": [
        "Now everything is ready to create the gallery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhij18mvdjkw"
      },
      "outputs": [],
      "source": [
        "gallery = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, labels in tqdm(loader_lfw_cropped):\n",
        "        image = image.to(device)\n",
        "        batch_embeddings = embedder(image)\n",
        "        gallery.extend(batch_embeddings.to('cpu').numpy())     \n",
        "\n",
        "gallery = np.asarray(gallery)\n",
        "print(f'The gallery has: {len(gallery)} samples.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7VoVnvGflYH"
      },
      "source": [
        "Once we have created the gallery, in order to find the identity of a new subject we can classify its embedding using a k-Nearest Neighbor search. The most basic approach is the *brute force* which involves the computation of distances between all the embeddings in the gallery against the new one. Contrary, we will use a `K-D tree`, i.e. a tree-based data structure that helps overcome the computational cost of the brute force approach, whose implementation is available in `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNpWESaPikWo"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def nearest_neighbor(sources: np.ndarray, \n",
        "                     targets: np.ndarray,\n",
        "                     num_neighbors: int,\n",
        "                     algorithm: str = 'kd_tree') -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Computes nearest neighbor search.\n",
        "\n",
        "    Estimates for each sample in source the nearest neighbor in target using \n",
        "    the specified algorithm.\n",
        "    \n",
        "    Args:\n",
        "        sources: the source samples.\n",
        "        targets: the target samples.\n",
        "        num_neighbors: the number of neighbors to find.\n",
        "        algorithm: the algorithm to use kd_tree or brute force.\n",
        "\n",
        "    Returns:\n",
        "        The euclidean distance from each sample in source to the nearest neighbor in target.\n",
        "        The indices of the nearest neighbor points on target for each sample in source.\n",
        "    \"\"\"    \n",
        "    kd_tree = NearestNeighbors(n_neighbors=num_neighbors, \n",
        "                               algorithm=algorithm, \n",
        "                               metric='euclidean')\n",
        "    kd_tree.fit(targets)\n",
        "    distances, indices = kd_tree.kneighbors(sources)\n",
        "\n",
        "    return distances, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg6_5-Eek4Y1"
      },
      "source": [
        "Let's do a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfA_NlOEj7DJ"
      },
      "outputs": [],
      "source": [
        "num_neighbors = 3\n",
        "idx_sanity = 7\n",
        "nn_distances, nn_indices = nearest_neighbor(gallery[idx_sanity].reshape(1, -1),\n",
        "                                            gallery,\n",
        "                                            num_neighbors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPlkZCyvsrEX"
      },
      "outputs": [],
      "source": [
        "def show_neighbors(query_image: PIL.Image.Image,                   \n",
        "                   indices: np.ndarray,\n",
        "                   distances: np.ndarray,\n",
        "                   dataset: torchvision.datasets,\n",
        "                   threshold: float,\n",
        "                   class_query: str = None,\n",
        "                   transform=None) -> None:\n",
        "    \"\"\"Shows the query image together with the found nearest neighbors.\n",
        "\n",
        "    Args:\n",
        "        query_image: the input image.        \n",
        "        indices: the indices of nearest neighbors.\n",
        "        distances: the distances of nearest neighbors.\n",
        "        dataset: the dataset on which the gallery was built.\n",
        "        threshold: the matching threshold\n",
        "        class_query: the name of the class of the query image.\n",
        "        transform: the transformation to apply to the image.        \n",
        "\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    distances = distances.squeeze()\n",
        "    indices = indices.squeeze()\n",
        "    num_images = len(indices) + 1\n",
        "\n",
        "    figure = plt.figure(figsize=(15, 5))\n",
        "    figure.add_subplot(1, num_images, 1)\n",
        "    plt.imshow(query_image)\n",
        "    class_query_title = class_query if class_query else '' \n",
        "    plt.title(f'Query \\n {class_query_title} ', color='g')\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    for idx, (idx_nn, dist_nn) in enumerate(zip(indices, distances)):        \n",
        "        figure.add_subplot(1, num_images, idx + 2)\n",
        "        image, label_matched = dataset[idx_nn]        \n",
        "        \n",
        "        if isinstance(label_matched, str):\n",
        "            class_match = label_matched.split('/')[-1].split('.')[-2]\n",
        "        else:\n",
        "            class_match = dataset.classes[label_matched]\n",
        "\n",
        "        if class_query is not None:\n",
        "            color = 'g' if class_query == class_match else 'r'\n",
        "        else:\n",
        "            color = 'g' if dist_nn <= threshold else 'r'\n",
        "\n",
        "        plt.title(f'{class_match} \\n Distance: {dist_nn:.3f}', color=color)\n",
        "        if transform:\n",
        "            image = transform(image)\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzSgZayLmcDL"
      },
      "outputs": [],
      "source": [
        "threshold = 0.85\n",
        "show_neighbors(dataset_lfw[idx_sanity][0], nn_indices, nn_distances, dataset_lfw, \n",
        "               threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_3ypVmEquab"
      },
      "source": [
        "Check one subject that is in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yepD6eZ3aF2J"
      },
      "outputs": [],
      "source": [
        "def get_image_from_url(image_url:str , mode:str ='RGB') -> PIL.Image.Image:\n",
        "    \"\"\"Downloads and opens the image from the url.\n",
        "\n",
        "    Args:\n",
        "        image_url: the url for the image.\n",
        "        mode: a string which defines the type and depth of a pixel in the image.\n",
        "\n",
        "    Returns:\n",
        "        The image read.\n",
        "    \"\"\"\n",
        "    response = requests.get(image_url, stream=True)    \n",
        "    return Image.open(response.raw).convert(mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_BvX9woY58U"
      },
      "outputs": [],
      "source": [
        "def detect_and_embed(image: PIL.Image.Image, \n",
        "                     detector: nn.Module, \n",
        "                     embedder: nn.Module) -> Tuple[PIL.Image.Image,\n",
        "                                                   np.ndarray]:\n",
        "    \"\"\"Detects the face from the image and then creates the embedding for it.\n",
        "\n",
        "    Args:\n",
        "        image: the input iamge.\n",
        "        detector: the face detector.\n",
        "        embedder: the embedder.\n",
        "\n",
        "    Returns:\n",
        "        The image of the cropped face.\n",
        "        The embedding for the face.\n",
        "    \"\"\"\n",
        "    detector.eval()\n",
        "    embedder.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_face = detector(image)\n",
        "        image_face_batch = image_face.unsqueeze(0)                \n",
        "        embedding = embedder(image_face_batch.to(device))                \n",
        "    \n",
        "    embedding = embedding.to('cpu').numpy()    \n",
        "    image_face = image_face.to('cpu')\n",
        "    image_face = (image_face * (128. / 255.)) + (127. / 255.)\n",
        "    image_face_pil = transforms.ToPILImage()(image_face)\n",
        "\n",
        "    return image_face_pil, embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKLW10hRsXNr"
      },
      "outputs": [],
      "source": [
        "image_url = \"https://www.automotorinews.it/wp-content/uploads/2022/04/Valentino-Rossi-1.jpg\"\n",
        "image = get_image_from_url(image_url)\n",
        "\n",
        "image_face, embedding = detect_and_embed(image, detector, embedder)\n",
        "\n",
        "nn_distances, nn_indices = nearest_neighbor(embedding,\n",
        "                                            gallery,\n",
        "                                            num_neighbors)\n",
        "show_neighbors(image_face, \n",
        "               nn_indices, \n",
        "               nn_distances, \n",
        "               dataset_lfw,\n",
        "               threshold)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BnZa6CH5GxxJ",
        "Ja5-EeRoueLz",
        "KYamN4SXuWoB",
        "T8O7168XuraN",
        "dtKtKw6Uvbw1",
        "8CfIJZM5mk9H",
        "MrOo8FIyvWxd"
      ],
      "name": "CVLab6-MetricLearning-FACE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
